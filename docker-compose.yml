services:
  api:
    build:
      context: .
      dockerfile: ./Dockerfile.api
    container_name: api
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
      - ./agent_config.yaml:/app/agent_config.yaml
    env_file:
      - .env
    environment:
      - AGENT_ENABLED=true
      - AGENT_CHECK_INTERVAL=5
    depends_on:
      - redis
      - ollama
    restart: unless-stopped

  worker:
    build:
      context: .
      dockerfile: ./Dockerfile.worker
    container_name: worker
    command: celery -A app.worker.celery_app worker --loglevel=info --queues=emails,agent,replies
    volumes:
      - ./data:/app/data
      - ./agent_config.yaml:/app/agent_config.yaml
      - /var/run/docker.sock:/var/run/docker.sock  # For Docker control
    env_file:
      - .env
    environment:
      - AGENT_ENABLED=true
    depends_on:
      - redis
      - ollama
    restart: unless-stopped

  beat:
    build:
      context: .
      dockerfile: ./Dockerfile.worker
    container_name: beat
    command: celery -A app.worker.celery_app beat --loglevel=info
    volumes:
      - ./data:/app/data
      - ./agent_config.yaml:/app/agent_config.yaml
    env_file:
      - .env
    environment:
      - AGENT_ENABLED=true
    depends_on:
      - redis
    restart: unless-stopped

  redis:
    image: redis:7
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    restart: unless-stopped

  ollama:
    image: ollama/ollama
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    restart: unless-stopped
    command: ["serve"]

  lead-scraper:
    build:
      context: .
      dockerfile: Dockerfile.scraper
    container_name: lead-scraper
    command: python lead_scraper/continuous_scraper.py
    volumes:
      - ./lead_scraper:/app/lead_scraper
      - ./data:/app/data
    environment:
      - REDIS_URL=redis://redis:6379/0 
      - AUTO_PUSH_TO_QUEUE=true 
    depends_on:
      - redis
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================
  # ‚úÖ AUTO-IMPORT SERVICE
  # Runs once on startup to import scraped leads
  # ‚ö†Ô∏è  With 5,200 leads, this takes ~5-10 minutes
  # ============================================
  auto-import:
    build:
      context: .
      dockerfile: ./Dockerfile.api  # Reuse API dockerfile
    container_name: auto-import
    volumes:
      - ./data:/app/data
      - ./auto_import_scraped_leads.py:/app/auto_import_scraped_leads.py
    env_file:
      - .env
    depends_on:
      - api
      - redis
    command: >
      sh -c "
        echo '‚è≥ Waiting 15 seconds for API to be ready...' &&
        sleep 15 &&
        echo 'ü§ñ Starting auto-import of scraped leads...' &&
        echo '‚ö†Ô∏è  Large import (5,200+ leads) may take 5-10 minutes' &&
        python /app/auto_import_scraped_leads.py &&
        echo '‚úÖ Auto-import complete! Container will exit.' &&
        exit 0
      "
    restart: "no"  # Run once per docker-compose up

# ============================================
# üîª REQUIRED TOP-LEVEL VOLUME DEFINITIONS
# ============================================
volumes:
  ollama:
  redis-data: